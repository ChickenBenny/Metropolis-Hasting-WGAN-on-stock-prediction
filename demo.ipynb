{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(x, y, seq_length):\n",
    "    x_windows = []\n",
    "    y_windows = []\n",
    "    for i in range(seq_length, x.shape[0]):\n",
    "        x_windows.append(x[i-seq_length:i, :])\n",
    "        y_windows.append(y[i-seq_length:i+1])\n",
    "    return np.array(x_windows), np.array(y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX: (2707, 57) trainY: (2707,)\n",
      "testX: (319, 57) testY: (319,)\n",
      "Training the VAE model------------------\n",
      "Epoch: 1/250, Loss: 769.5591\n",
      "Epoch: 2/250, Loss: 708.2346\n",
      "Epoch: 3/250, Loss: 640.2910\n",
      "Epoch: 4/250, Loss: 532.6404\n",
      "Epoch: 5/250, Loss: 452.7827\n",
      "Epoch: 6/250, Loss: 356.1686\n",
      "Epoch: 7/250, Loss: 325.9710\n",
      "Epoch: 8/250, Loss: 243.4519\n",
      "Epoch: 9/250, Loss: 223.8246\n",
      "Epoch: 10/250, Loss: 168.1114\n",
      "Epoch: 11/250, Loss: 113.1002\n",
      "Epoch: 12/250, Loss: 114.8276\n",
      "Epoch: 13/250, Loss: 118.2852\n",
      "Epoch: 14/250, Loss: 103.1270\n",
      "Epoch: 15/250, Loss: 77.3466\n",
      "Epoch: 16/250, Loss: 60.9898\n",
      "Epoch: 17/250, Loss: 62.6590\n",
      "Epoch: 18/250, Loss: 73.8302\n",
      "Epoch: 19/250, Loss: 81.3866\n",
      "Epoch: 20/250, Loss: 81.1956\n",
      "Epoch: 21/250, Loss: 77.5761\n",
      "Epoch: 22/250, Loss: 72.4586\n",
      "Epoch: 23/250, Loss: 66.1455\n",
      "Epoch: 24/250, Loss: 60.9336\n",
      "Epoch: 25/250, Loss: 59.5793\n",
      "Epoch: 26/250, Loss: 62.2905\n",
      "Epoch: 27/250, Loss: 67.4894\n",
      "Epoch: 28/250, Loss: 72.1275\n",
      "Epoch: 29/250, Loss: 74.0820\n",
      "Epoch: 30/250, Loss: 75.4756\n",
      "Epoch: 31/250, Loss: 74.4106\n",
      "Epoch: 32/250, Loss: 69.5446\n",
      "Epoch: 33/250, Loss: 61.3803\n",
      "Epoch: 34/250, Loss: 52.1329\n",
      "Epoch: 35/250, Loss: 44.5104\n",
      "Epoch: 36/250, Loss: 41.0037\n",
      "Epoch: 37/250, Loss: 41.3589\n",
      "Epoch: 38/250, Loss: 42.5581\n",
      "Epoch: 39/250, Loss: 44.1981\n",
      "Epoch: 40/250, Loss: 46.4809\n",
      "Epoch: 41/250, Loss: 49.6072\n",
      "Epoch: 42/250, Loss: 53.9039\n",
      "Epoch: 43/250, Loss: 59.4935\n",
      "Epoch: 44/250, Loss: 66.1465\n",
      "Epoch: 45/250, Loss: 73.6407\n",
      "Epoch: 46/250, Loss: 81.5608\n",
      "Epoch: 47/250, Loss: 89.5227\n",
      "Epoch: 48/250, Loss: 97.1991\n",
      "Epoch: 49/250, Loss: 104.5144\n",
      "Epoch: 50/250, Loss: 111.4868\n",
      "Epoch: 51/250, Loss: 118.0809\n",
      "Epoch: 52/250, Loss: 124.3372\n",
      "Epoch: 53/250, Loss: 129.9249\n",
      "Epoch: 54/250, Loss: 134.7928\n",
      "Epoch: 55/250, Loss: 138.6898\n",
      "Epoch: 56/250, Loss: 141.5644\n",
      "Epoch: 57/250, Loss: 143.2829\n",
      "Epoch: 58/250, Loss: 144.0192\n",
      "Epoch: 59/250, Loss: 143.7749\n",
      "Epoch: 60/250, Loss: 142.8303\n",
      "Epoch: 61/250, Loss: 141.5290\n",
      "Epoch: 62/250, Loss: 139.9697\n",
      "Epoch: 63/250, Loss: 138.2937\n",
      "Epoch: 64/250, Loss: 136.3565\n",
      "Epoch: 65/250, Loss: 134.2137\n",
      "Epoch: 66/250, Loss: 131.7285\n",
      "Epoch: 67/250, Loss: 128.9723\n",
      "Epoch: 68/250, Loss: 125.6603\n",
      "Epoch: 69/250, Loss: 121.8052\n",
      "Epoch: 70/250, Loss: 117.4526\n",
      "Epoch: 71/250, Loss: 112.5367\n",
      "Epoch: 72/250, Loss: 107.4247\n",
      "Epoch: 73/250, Loss: 102.2179\n",
      "Epoch: 74/250, Loss: 97.2657\n",
      "Epoch: 75/250, Loss: 92.5119\n",
      "Epoch: 76/250, Loss: 88.1747\n",
      "Epoch: 77/250, Loss: 84.1917\n",
      "Epoch: 78/250, Loss: 80.7941\n",
      "Epoch: 79/250, Loss: 77.9189\n",
      "Epoch: 80/250, Loss: 75.5457\n",
      "Epoch: 81/250, Loss: 73.8429\n",
      "Epoch: 82/250, Loss: 72.7497\n",
      "Epoch: 83/250, Loss: 72.3492\n",
      "Epoch: 84/250, Loss: 72.7350\n",
      "Epoch: 85/250, Loss: 73.7544\n",
      "Epoch: 86/250, Loss: 75.4878\n",
      "Epoch: 87/250, Loss: 77.6519\n",
      "Epoch: 88/250, Loss: 80.3370\n",
      "Epoch: 89/250, Loss: 82.7987\n",
      "Epoch: 90/250, Loss: 84.8776\n",
      "Epoch: 91/250, Loss: 86.8484\n",
      "Epoch: 92/250, Loss: 88.3695\n",
      "Epoch: 93/250, Loss: 89.2968\n",
      "Epoch: 94/250, Loss: 89.8387\n",
      "Epoch: 95/250, Loss: 89.9800\n",
      "Epoch: 96/250, Loss: 89.8098\n",
      "Epoch: 97/250, Loss: 89.5720\n",
      "Epoch: 98/250, Loss: 89.4834\n",
      "Epoch: 99/250, Loss: 89.6546\n",
      "Epoch: 100/250, Loss: 90.2699\n",
      "Epoch: 101/250, Loss: 91.2187\n",
      "Epoch: 102/250, Loss: 92.5858\n",
      "Epoch: 103/250, Loss: 94.0689\n",
      "Epoch: 104/250, Loss: 95.6393\n",
      "Epoch: 105/250, Loss: 97.0445\n",
      "Epoch: 106/250, Loss: 98.1288\n",
      "Epoch: 107/250, Loss: 98.7945\n",
      "Epoch: 108/250, Loss: 98.9746\n",
      "Epoch: 109/250, Loss: 98.6503\n",
      "Epoch: 110/250, Loss: 97.9440\n",
      "Epoch: 111/250, Loss: 97.1193\n",
      "Epoch: 112/250, Loss: 96.3353\n",
      "Epoch: 113/250, Loss: 96.0809\n",
      "Epoch: 114/250, Loss: 96.4837\n",
      "Epoch: 115/250, Loss: 97.5755\n",
      "Epoch: 116/250, Loss: 99.8287\n",
      "Epoch: 117/250, Loss: 102.9754\n",
      "Epoch: 118/250, Loss: 107.0213\n",
      "Epoch: 119/250, Loss: 111.8658\n",
      "Epoch: 120/250, Loss: 117.1576\n",
      "Epoch: 121/250, Loss: 122.4806\n",
      "Epoch: 122/250, Loss: 127.6505\n",
      "Epoch: 123/250, Loss: 132.3711\n",
      "Epoch: 124/250, Loss: 136.2533\n",
      "Epoch: 125/250, Loss: 139.1592\n",
      "Epoch: 126/250, Loss: 140.7464\n",
      "Epoch: 127/250, Loss: 141.1864\n",
      "Epoch: 128/250, Loss: 140.3239\n",
      "Epoch: 129/250, Loss: 138.4902\n",
      "Epoch: 130/250, Loss: 135.7222\n",
      "Epoch: 131/250, Loss: 132.7948\n",
      "Epoch: 132/250, Loss: 128.5988\n",
      "Epoch: 133/250, Loss: 124.8038\n",
      "Epoch: 134/250, Loss: 121.0481\n",
      "Epoch: 135/250, Loss: 117.5229\n",
      "Epoch: 136/250, Loss: 114.3215\n",
      "Epoch: 137/250, Loss: 111.4047\n",
      "Epoch: 138/250, Loss: 108.7605\n",
      "Epoch: 139/250, Loss: 106.2904\n",
      "Epoch: 140/250, Loss: 103.9682\n",
      "Epoch: 141/250, Loss: 101.7356\n",
      "Epoch: 142/250, Loss: 99.5825\n",
      "Epoch: 143/250, Loss: 97.2274\n",
      "Epoch: 144/250, Loss: 94.8695\n",
      "Epoch: 145/250, Loss: 92.5133\n",
      "Epoch: 146/250, Loss: 89.9598\n",
      "Epoch: 147/250, Loss: 87.4617\n",
      "Epoch: 148/250, Loss: 85.0410\n",
      "Epoch: 149/250, Loss: 82.6086\n",
      "Epoch: 150/250, Loss: 80.5196\n",
      "Epoch: 151/250, Loss: 78.8127\n",
      "Epoch: 152/250, Loss: 77.6639\n",
      "Epoch: 153/250, Loss: 77.1788\n",
      "Epoch: 154/250, Loss: 77.5295\n",
      "Epoch: 155/250, Loss: 78.6267\n",
      "Epoch: 156/250, Loss: 80.3660\n",
      "Epoch: 157/250, Loss: 82.8290\n",
      "Epoch: 158/250, Loss: 85.7812\n",
      "Epoch: 159/250, Loss: 89.2923\n",
      "Epoch: 160/250, Loss: 93.2846\n",
      "Epoch: 161/250, Loss: 97.6135\n",
      "Epoch: 162/250, Loss: 102.2538\n",
      "Epoch: 163/250, Loss: 106.9227\n",
      "Epoch: 164/250, Loss: 111.6609\n",
      "Epoch: 165/250, Loss: 116.1777\n",
      "Epoch: 166/250, Loss: 120.2573\n",
      "Epoch: 167/250, Loss: 123.8687\n",
      "Epoch: 168/250, Loss: 126.8887\n",
      "Epoch: 169/250, Loss: 129.0136\n",
      "Epoch: 170/250, Loss: 130.3721\n",
      "Epoch: 171/250, Loss: 130.9570\n",
      "Epoch: 172/250, Loss: 130.7579\n",
      "Epoch: 173/250, Loss: 130.0400\n",
      "Epoch: 174/250, Loss: 128.9697\n",
      "Epoch: 175/250, Loss: 127.5636\n",
      "Epoch: 176/250, Loss: 125.9871\n",
      "Epoch: 177/250, Loss: 124.3667\n",
      "Epoch: 178/250, Loss: 122.9068\n",
      "Epoch: 179/250, Loss: 121.6626\n",
      "Epoch: 180/250, Loss: 120.8109\n",
      "Epoch: 181/250, Loss: 120.4533\n",
      "Epoch: 182/250, Loss: 120.4957\n",
      "Epoch: 183/250, Loss: 120.9311\n",
      "Epoch: 184/250, Loss: 121.6961\n",
      "Epoch: 185/250, Loss: 122.5619\n",
      "Epoch: 186/250, Loss: 123.4537\n",
      "Epoch: 187/250, Loss: 124.0676\n",
      "Epoch: 188/250, Loss: 124.0308\n",
      "Epoch: 189/250, Loss: 123.4502\n",
      "Epoch: 190/250, Loss: 121.9823\n",
      "Epoch: 191/250, Loss: 119.6871\n",
      "Epoch: 192/250, Loss: 116.5133\n",
      "Epoch: 193/250, Loss: 112.6394\n",
      "Epoch: 194/250, Loss: 108.1447\n",
      "Epoch: 195/250, Loss: 103.4195\n",
      "Epoch: 196/250, Loss: 98.5897\n",
      "Epoch: 197/250, Loss: 94.1667\n",
      "Epoch: 198/250, Loss: 90.2734\n",
      "Epoch: 199/250, Loss: 87.4788\n",
      "Epoch: 200/250, Loss: 85.7054\n",
      "Epoch: 201/250, Loss: 85.2479\n",
      "Epoch: 202/250, Loss: 86.3085\n",
      "Epoch: 203/250, Loss: 88.6145\n",
      "Epoch: 204/250, Loss: 92.3420\n",
      "Epoch: 205/250, Loss: 97.0861\n",
      "Epoch: 206/250, Loss: 102.6998\n",
      "Epoch: 207/250, Loss: 108.9124\n",
      "Epoch: 208/250, Loss: 115.3657\n",
      "Epoch: 209/250, Loss: 121.6666\n",
      "Epoch: 210/250, Loss: 127.4703\n",
      "Epoch: 211/250, Loss: 132.5156\n",
      "Epoch: 212/250, Loss: 136.3487\n",
      "Epoch: 213/250, Loss: 139.0508\n",
      "Epoch: 214/250, Loss: 140.2197\n",
      "Epoch: 215/250, Loss: 140.1092\n",
      "Epoch: 216/250, Loss: 138.7477\n",
      "Epoch: 217/250, Loss: 136.3224\n",
      "Epoch: 218/250, Loss: 133.2201\n",
      "Epoch: 219/250, Loss: 129.7339\n",
      "Epoch: 220/250, Loss: 126.0657\n",
      "Epoch: 221/250, Loss: 122.7178\n",
      "Epoch: 222/250, Loss: 119.7103\n",
      "Epoch: 223/250, Loss: 117.3867\n",
      "Epoch: 224/250, Loss: 115.5622\n",
      "Epoch: 225/250, Loss: 114.4618\n",
      "Epoch: 226/250, Loss: 113.9442\n",
      "Epoch: 227/250, Loss: 113.7593\n",
      "Epoch: 228/250, Loss: 113.9209\n",
      "Epoch: 229/250, Loss: 114.2340\n",
      "Epoch: 230/250, Loss: 114.5320\n",
      "Epoch: 231/250, Loss: 114.6152\n",
      "Epoch: 232/250, Loss: 114.3179\n",
      "Epoch: 233/250, Loss: 113.5262\n",
      "Epoch: 234/250, Loss: 112.1008\n",
      "Epoch: 235/250, Loss: 110.0344\n",
      "Epoch: 236/250, Loss: 107.2413\n",
      "Epoch: 237/250, Loss: 103.9576\n",
      "Epoch: 238/250, Loss: 100.2903\n",
      "Epoch: 239/250, Loss: 96.5730\n",
      "Epoch: 240/250, Loss: 93.1859\n",
      "Epoch: 241/250, Loss: 90.3744\n",
      "Epoch: 242/250, Loss: 88.6444\n",
      "Epoch: 243/250, Loss: 88.0582\n",
      "Epoch: 244/250, Loss: 89.0391\n",
      "Epoch: 245/250, Loss: 91.4468\n",
      "Epoch: 246/250, Loss: 95.3758\n",
      "Epoch: 247/250, Loss: 100.7239\n",
      "Epoch: 248/250, Loss: 107.0282\n",
      "Epoch: 249/250, Loss: 114.2260\n",
      "Epoch: 250/250, Loss: 121.7596\n",
      "train_x: (2697, 10, 67), train_y: (2697, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./assets/data.csv', index_col='Date')\n",
    "data['y'] = data['Close']\n",
    "\n",
    "data_x = data.iloc[:, :-1].values\n",
    "data_y = data.iloc[:, -1].values\n",
    "\n",
    "train_x, test_x = data_x[:-319, :], data_x[-319:, :]\n",
    "train_y, test_y = data_y[:-319], data_y[-319:]\n",
    "\n",
    "print(f'trainX: {train_x.shape} trainY: {train_y.shape}')\n",
    "print(f'testX: {test_x.shape} testY: {test_y.shape}')\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "train_x = x_scaler.fit_transform(train_x)\n",
    "train_y = y_scaler.fit_transform(train_y.reshape(-1, 1))\n",
    "test_x = x_scaler.transform(test_x)\n",
    "test_y = y_scaler.transform(test_y.reshape(-1, 1))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float()),\n",
    "                          batch_size=128, shuffle=True)\n",
    "\n",
    "vae = VAE([train_x.shape[1], 400, 400, 400, 400, 10], 10, use_cuda=1)\n",
    "num_epochs = 250\n",
    "learning_rate = 0.0003\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Training the VAE model------------------')\n",
    "history = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(vae.device)\n",
    "        x_hat, z, mu, log_var = vae(x)\n",
    "        loss = vae.criterion(x_hat, x, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    history[epoch] = total_loss\n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}')\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, mu, _ = vae(torch.from_numpy(train_x).float().to(vae.device))\n",
    "    mu = mu.cpu().numpy()\n",
    "\n",
    "train_x = np.concatenate((train_x, mu), axis=1)\n",
    "\n",
    "seq_length = 10\n",
    "train_x, train_y = sliding_windows(train_x, train_y, seq_length)\n",
    "print(f'train_x: {train_x.shape}, train_y: {train_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] LossD: -0.00070 LossG:-0.39951\n",
      "[2/500] LossD: -0.00220 LossG:-0.40650\n",
      "[3/500] LossD: -0.00352 LossG:-0.41188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.000015\u001b[39m\n\u001b[0;32m      7\u001b[0m wgan\u001b[39m.\u001b[39moptimizers(lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m----> 8\u001b[0m wgan\u001b[39m.\u001b[39;49mtraining_step(num_epochs, train_wgan_data, seq_length)\n",
      "File \u001b[1;32md:\\Metropolis-Hasting-WGAN-on-stock-prediction\\models\\wgan.py:50\u001b[0m, in \u001b[0;36mWGAN.training_step\u001b[1;34m(self, epochs, train_dataloader, window_size, plot_loss)\u001b[0m\n\u001b[0;32m     48\u001b[0m loss_d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator_loss(cirtic_real, critic_fake)\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdis\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 50\u001b[0m loss_d\u001b[39m.\u001b[39;49mbackward(retain_graph \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_D\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     53\u001b[0m output_fake \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdis(fake_data)\n",
      "File \u001b[1;32mc:\\Users\\Benny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Benny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the WGAN model and will fine-tune the model later\n",
    "# Generate the data for training WGAN\n",
    "train_wgan_data = DataLoader(TensorDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float()), batch_size=128, shuffle=False)\n",
    "wgan = WGAN(67, 10, 1)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.000015\n",
    "wgan.optimizers(lr=learning_rate)\n",
    "wgan.training_step(num_epochs, train_wgan_data, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 11, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
